{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d290d1-8777-435f-a5e2-123d0c6e436b",
   "metadata": {},
   "source": [
    "## Library installation (only first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\descript_audiotools-0.7.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\flatten_dict-0.4.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\markdown2-2.4.13-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\protobuf-3.19.6-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pystoi-0.4.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\randomname-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\tensorboard-2.17.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch_stoi-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063fd768-2fe9-4315-b472-23d52a5c921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\descript_audiotools-0.7.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\flatten_dict-0.4.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\markdown2-2.4.13-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\protobuf-3.19.6-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pystoi-0.4.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\randomname-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\tensorboard-2.17.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch_stoi-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fce462e-0156-4751-aeba-df63a6cf37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\descript_audiotools-0.7.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\flatten_dict-0.4.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\markdown2-2.4.13-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\protobuf-3.19.6-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pystoi-0.4.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\randomname-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\tensorboard-2.17.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch_stoi-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\descript_audiotools-0.7.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\flatten_dict-0.4.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\markdown2-2.4.13-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\protobuf-3.19.6-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pystoi-0.4.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\randomname-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\tensorboard-2.17.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "DEPRECATION: Loading egg at c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\torch_stoi-0.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45523688-2323-47c6-a4c3-74441a7d68b0",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab574a30-36f7-46f2-8b5c-df8de3bc784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e449586-6df0-4775-8c09-de9aa1c99c0f",
   "metadata": {},
   "source": [
    "# Import AST Pretrained and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf758e99-f37f-4e8c-9f30-6a0ae431333f",
   "metadata": {},
   "source": [
    "## Import dataset huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b93ab4-bc00-4a86-9ee3-c6934a7b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_huggingface = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset_huggingface = dataset_huggingface.sort(\"id\")\n",
    "sampling_rate = dataset_huggingface.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91f313-1b2c-4fdb-beb4-e3d1436c2286",
   "metadata": {},
   "source": [
    "## Import AST huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dfc551-2442-4274-a0a4-ed1281707bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c5bc6-a339-4c1f-ae24-9ec4641ca4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTSdpaAttention(\n",
       "            (attention): ASTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ast pretrained\n",
    "ast_huggingface = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de9e3-7f7f-40b2-911c-92d3d4013896",
   "metadata": {},
   "source": [
    "## Test pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4185ed5-b0e3-4b3c-82f1-4bc82a1c423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = ast_huggingface(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = ast_huggingface.config.id2label[predicted_class_ids]\n",
    "print(predicted_label)\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = ast_huggingface.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([ast_huggingface.config.label2id[target_label]])\n",
    "loss = ast_huggingface(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578e51-5352-4509-90ad-a8ee869c69c6",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcd6f0-f65a-44b9-b327-3096e50b4a89",
   "metadata": {},
   "source": [
    "## Retrieve Output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "187b8b8f-f1f4-4ffd-8ce1-e7c11fdd2eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTModel(\n",
       "  (embeddings): ASTEmbeddings(\n",
       "    (patch_embeddings): ASTPatchEmbeddings(\n",
       "      (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ASTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ASTLayer(\n",
       "        (attention): ASTSdpaAttention(\n",
       "          (attention): ASTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ASTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ASTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ASTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0265fcd-366f-4fa5-9cd1-8204a8ffe4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1214, 768]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = ast_model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9207ad-fdde-42f1-b9b8-263d20db75e5",
   "metadata": {},
   "source": [
    "## Model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7892cd7b-906c-4c63-baaa-452420a9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST_PromptTuning(nn.Module):\n",
    "\n",
    "    # dropout apply dropout after each prompt\n",
    "    # str = \"none\" --> only head tuning\n",
    "    def __init__(self, prompt_tokens: int = 5, prompt_dropout: float = 0.0, prompt_type: str = 'deep'):\n",
    "        super().__init__()\n",
    "\n",
    "        # load vit model\n",
    "        self.encoder = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        # hidden_size = depth of the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 384),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 192),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 96),\n",
    "            nn.Linear(384, 15)\n",
    "        )\n",
    "\n",
    "        # freeze\n",
    "        for n, p in self.encoder.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.prompt_type = prompt_type # \"shallow\" \"deep\" or None\n",
    "\n",
    "        if prompt_type is not None:\n",
    "\n",
    "            # prompt\n",
    "            self.prompt_tokens = prompt_tokens  # number of prompted tokens\n",
    "            self.prompt_dropout = nn.Dropout(prompt_dropout)\n",
    "            self.prompt_dim = self.encoder.config.hidden_size\n",
    "\n",
    "            # initiate prompt (random)\n",
    "            val = math.sqrt(6. / float(3 * reduce(mul, (self.encoder.config.patch_size, self.encoder.config.patch_size), 1) + self.prompt_dim))\n",
    "\n",
    "            # my vector of learnable parameters (how many (prompt_tokens) and dimension (prompt_dim))\n",
    "            self.prompt_embeddings = nn.Parameter(torch.zeros(1, self.prompt_tokens, self.prompt_dim))\n",
    "\n",
    "            # xavier_uniform initialization\n",
    "            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n",
    "\n",
    "            if self.prompt_type == 'deep':\n",
    "                self.total_d_layer = self.encoder.config.num_hidden_layers\n",
    "                self.deep_prompt_embeddings = nn.Parameter(\n",
    "                    # - 1 cause shallow already inserted\n",
    "                    torch.zeros(self.total_d_layer-1, self.prompt_tokens, self.prompt_dim)\n",
    "                )\n",
    "                # xavier_uniform initialization\n",
    "                nn.init.uniform_(self.deep_prompt_embeddings.data, -val, val)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # set train status for this class: disable all but the prompt-related modules\n",
    "        if mode:\n",
    "            # training:\n",
    "            self.encoder.eval()\n",
    "            if self.prompt_type is not None:\n",
    "              # enable dropout and batch normalization\n",
    "                self.prompt_dropout.train()\n",
    "        else:\n",
    "            # eval:\n",
    "            for module in self.children():\n",
    "                module.train(mode)\n",
    "\n",
    "    def incorporate_prompt(self, x, prompt_embeddings, n_prompt: int = 0):\n",
    "        # x shape: (batch size, n_tokens, hidden_dim)\n",
    "        # pompt_embeddings shape: (1, n_prompt, hidden_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # peek the class token, add prompts, add sequence\n",
    "\n",
    "        # concat prompts: (batch size, cls_token + n_prompt + n_patches, hidden_dim)\n",
    "        x = torch.cat((\n",
    "            x[:, :1, :],\n",
    "            self.prompt_dropout(prompt_embeddings.expand(B, -1, -1)),\n",
    "            x[:, (1+n_prompt):, :]\n",
    "        ), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "\n",
    "        # go through the encoder embeddings\n",
    "        x = self.encoder.embeddings(x)\n",
    "\n",
    "        # add prompts\n",
    "        x = self.incorporate_prompt(x, self.prompt_embeddings)\n",
    "\n",
    "        if self.prompt_type == 'deep':\n",
    "            # deep mode\n",
    "            x = self.encoder.encoder.layer[0](x)[0]\n",
    "            for i in range(1, self.total_d_layer):\n",
    "                x = self.incorporate_prompt(x, self.deep_prompt_embeddings[i-1], self.prompt_tokens)\n",
    "                x = model.encoder.encoder.layer[i](x)[0]\n",
    "        else:\n",
    "            # shallow mode\n",
    "            x = self.encoder.encoder(x)[\"last_hidden_state\"]\n",
    "\n",
    "        x = self.encoder.layernorm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.prompt_type is not None:\n",
    "            x = self.forward_features(x)[:, 0, :]\n",
    "        else:\n",
    "          # pass x, take the classification token\n",
    "            x = self.encoder(x)[\"last_hidden_state\"][:, 0, :]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5f66293-d648-4fb4-808d-1ef178a2e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST params: 86488335\n",
      "Head fine-tuning: 301071\n",
      "Shallow prompt-tuning: 304911\n",
      "Deep prompt-tuning: 347151\n"
     ]
    }
   ],
   "source": [
    "ast_prompt = AST_PromptTuning(prompt_type=None)\n",
    "# count number of parameters\n",
    "print(\"AST params:\", sum(p.numel() for p in ast_prompt.parameters()))\n",
    "# count number of trainable parameters\n",
    "print(\"Head fine-tuning:\", sum(p.numel() for p in ast_prompt.parameters() if p.requires_grad))\n",
    "ast_prompt_shallow = AST_PromptTuning(prompt_type='shallow')\n",
    "# count number of trainable parameters\n",
    "print(\"Shallow prompt-tuning:\", sum(p.numel() for p in ast_prompt_shallow.parameters() if p.requires_grad))\n",
    "ast_prompt_deep = AST_PromptTuning(prompt_type='deep')\n",
    "# count number of trainable parameters\n",
    "print(\"Deep prompt-tuning:\", sum(p.numel() for p in ast_prompt_deep.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a071ebf6-97fa-4c54-9831-12b5dbdd824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ast_prompt(inputs['input_values'])\n",
    "\n",
    "predicted_class_ids = torch.argmax(outputs, dim=-1).item()\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ca634f-8544-499d-b268-8383356231ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0357, 0.0523, 0.1290, 0.0637, 0.1452, 0.0434, 0.0499, 0.0458, 0.0540,\n",
       "         0.0661, 0.0557, 0.0880, 0.0422, 0.0652, 0.0638]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = F.softmax(outputs, dim=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bebb3-793a-4215-b567-97907f86307a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a87894-a860-4c30-a814-9119f3cedb47",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "370425c2-b8aa-44bb-bf95-07aa08e059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoFeatureExtractor wants in input an array that contains the audio in format .flac --> this function convert a raw audio in .flac format\n",
    "def load_audio(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# this function returns the audio batch preprocessed\n",
    "def feature_extractor_batch_data(batch):\n",
    "    batch_feature_extractor = []\n",
    "    for index in range(0, len(batch[\"audio\"])):\n",
    "        output = feature_extractor(batch[\"audio\"][index], sampling_rate=batch[\"sample_rate\"][index], return_tensors=\"pt\")\n",
    "        batch_feature_extractor.append(output['input_values'])\n",
    "    # model wants in input a tensor with shape [num_batch, num_frame, num_mel]\n",
    "    batch_audio = torch.stack(batch_feature_extractor) # stacvk all the audio in a tensor in batch size\n",
    "    # prepare output\n",
    "    batch[\"audio\"] = batch_audio\n",
    "    return batch[\"audio\"], torch.tensor(batch[\"label\"])\n",
    "\n",
    "# test balanced dataset\n",
    "def count_class_presence(path):\n",
    "    class_count = {}\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            audio, label = line.strip().split()\n",
    "            if label in class_count:\n",
    "                class_count[label] += 1\n",
    "            else:\n",
    "                class_count[label] = 1\n",
    "    return class_count\n",
    "\n",
    "# model wants in input an integer for each label --> this function create a dictionary that maps each label to an index\n",
    "def create_dict_label(path):\n",
    "    class_dict = {}\n",
    "    label_index = 0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            _, class_name = line.strip().split()\n",
    "            if class_name not in class_dict:\n",
    "                class_dict[class_name] = label_index\n",
    "                label_index += 1\n",
    "    return class_dict\n",
    "\n",
    "# retrieve index of given label\n",
    "def from_label_to_index(label, dict_label):\n",
    "    return dict_label[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968417d-4032-4d6f-8d10-60063e4ff6db",
   "metadata": {},
   "source": [
    "## TUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b89526e-dc00-4ffc-a07f-566c0d1f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from folder to PyTorch Dataset\n",
    "class TUT17(Dataset):\n",
    "    def __init__(self, root_dir, audio_folder, label_folder, label_filename, split = 'train', seed = 42, val_frac= 0.1, test_frac= 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # we use seed because every time we instantiate the dataset we shuffle all the data\n",
    "        # we call at least 3 times (train, validation, test) --> overlapping area\n",
    "        # with seed we are sure that the dataset is shuffled always in the same way\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # store path audio and label\n",
    "        self.label_path = os.path.join(os.path.join(root_dir, label_folder), label_filename)\n",
    "        self.audio_path = os.path.join(root_dir, audio_folder)\n",
    "\n",
    "        # create dict label\n",
    "        self.class_dict = create_dict_label(self.label_path)\n",
    "        # retrive audio files\n",
    "        audio_names = os.listdir(self.audio_path)\n",
    "\n",
    "        # split dataset (percentage)\n",
    "        num_val = int(len(audio_names)*val_frac)\n",
    "        num_test = int(len(audio_names)*test_frac)\n",
    "        num_train = len(audio_names) - num_val - num_test\n",
    "\n",
    "        random.shuffle(audio_names)\n",
    "    \n",
    "        # split dataset (indexes)\n",
    "        if split == 'train':\n",
    "            self.data = audio_names[:num_train]\n",
    "        elif split == 'val':\n",
    "            self.data = audio_names[num_train:num_train+num_val]\n",
    "        elif split == 'test':\n",
    "            self.data = audio_names[-num_test:]\n",
    "        else:\n",
    "          raise ValueError('Invalid split value.')\n",
    "    \n",
    "    # optional\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_path, self.data[idx])\n",
    "        audio_name = audio_path.split('/')[-1][6:].replace('\\\\', '/').split('/')[-1]\n",
    "        with open(self.label_path, \"r\") as f:\n",
    "            while line := f.readline():\n",
    "                if line.split('\\t')[0][6:] == audio_name:\n",
    "                    f.close()\n",
    "                    audio, sample_rate = load_audio(audio_path)\n",
    "                    return {'audio': audio, 'sample_rate': sample_rate, 'label': from_label_to_index(line.split('\\t')[1][:-1], self.class_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37473dd-0577-4cdc-97f6-de8d4662d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='train')\n",
    "val_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='val')\n",
    "test_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='test')\n",
    "\n",
    "# train_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='train')\n",
    "# val_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='val')\n",
    "# test_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2c9675-88c7-4189-a173-da28bf87fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=4, num_workers=0, shuffle=False, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=4, num_workers=0, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e2902f-f1b4-4f21-a63e-ff2c90cccf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[ 4.6063e-05, -1.4133e-03, -2.7816e-03,  ..., -9.3138e-04,\n",
       "          -1.4768e-03,  0.0000e+00],\n",
       "         [-6.7040e-03, -1.0935e-02, -9.5532e-03,  ...,  6.0350e-03,\n",
       "           7.3235e-03,  0.0000e+00],\n",
       "         [-5.9278e-04, -4.1136e-04, -1.8220e-04,  ..., -7.7371e-04,\n",
       "          -3.7326e-04,  0.0000e+00],\n",
       "         [ 6.1991e-05, -2.8849e-04,  1.2753e-04,  ...,  2.4282e-03,\n",
       "           2.7198e-03,  0.0000e+00]]),\n",
       " 'sample_rate': tensor([16000, 16000, 16000, 16000]),\n",
       " 'label': tensor([14,  9,  1,  9])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f191b1-4f7e-46e9-a07c-955634e586e8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "313650d0-9f4b-48aa-9119-c1becea13199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "041bfcca-8a9a-410d-80e0-a2e2998a51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(val_loader):\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cd60963-7457-492a-9392-4b5f1b7dba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    labels_l = []\n",
    "    predictions_l = []\n",
    "    for batch in tqdm(test_loader):\n",
    "         # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        \n",
    "        # compute accuracy\n",
    "        labels_l.append(labels)\n",
    "        predictions_l.append(predictions)\n",
    "\n",
    "    labels = torch.cat(labels_l, dim=0)\n",
    "    predictions = torch.cat(predictions_l, dim=0)\n",
    "    \n",
    "    accuracy = (predictions == labels).sum().item() / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95832ebb-acda-4849-87e0-9c144c0157f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device, n_epochs: int = 10):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} : Train Loss {train_loss:.4f} : Val Loss {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c893d9-5497-4b25-9a82-589420bdddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70402ace076c457b8d11c60521bea1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f8a6e478304fa2a384bbc161598216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cerru\\AppData\\Local\\Temp\\ipykernel_1536\\3533858253.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return batch[\"audio\"], torch.tensor(batch[\"label\"])\n",
      "C:\\Users\\cerru\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:187: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e03abc085394264aab0370c2a07dc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 : Train Loss 1.1297 : Val Loss 0.6366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bc08dbf4d145499f110f7d9789d8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d0a3a07ff54e929705341bc3b37819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 : Train Loss 0.4474 : Val Loss 0.4685\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956eb1e3052c4caa9c6b06f215999147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ac7da122b14f2689f029c8b220ed20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 : Train Loss 0.3050 : Val Loss 0.3980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757971f5952d4cdc944100d7677ce471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7d613bc3b0417ab18a8ed5395383e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 : Train Loss 0.2268 : Val Loss 0.3432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631f3d97ebec434ba29cbd149d5ddb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ast_prompt = AST_PromptTuning(prompt_type=None)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_normal = torch.optim.Adam(ast_prompt.parameters(), lr=0.0001)\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "\n",
    "train(ast_prompt, train_loader, val_loader, criterion, optimizer_normal, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b4a8607-b0b5-4db2-807f-86e8314eeea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8322679b3e994d83a27cc919dce3020f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cerru\\AppData\\Local\\Temp\\ipykernel_6412\\1260772686.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return batch[\"audio\"], torch.tensor(batch[\"label\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160])\n",
      "torch.Size([160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9125"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(ast_prompt, test_loader, criterion, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d3093-6b94-4dce-ad21-bd9810422d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt_shallow = AST_PromptTuning(prompt_type='shallow')\n",
    "\n",
    "optimizer_shallow = torch.optim.Adam(ast_prompt_shallow.parameters(), lr=0.0001)\n",
    "\n",
    "train(ast_prompt, train_loader, val_loader, criterion, optimizer_shallow, dev)\n",
    "test(ast_prompt, test_loader, criterion, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c6751-5451-4dac-9014-79c417c3105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt_deep = AST_PromptTuning(prompt_type='deep')\n",
    "\n",
    "optimizer_deep = torch.optim.Adam(ast_prompt_deep.parameters(), lr=0.0001)\n",
    "\n",
    "train(ast_prompt_deep, train_loader, val_loader, criterion, optimizer_deep, dev)\n",
    "test(ast_prompt_deep, test_loader, criterion, dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
