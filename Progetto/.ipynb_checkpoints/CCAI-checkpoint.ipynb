{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inizializzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063fd768-2fe9-4315-b472-23d52a5c921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fce462e-0156-4751-aeba-df63a6cf37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cerru\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e449586-6df0-4775-8c09-de9aa1c99c0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import AST Pretrained and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf758e99-f37f-4e8c-9f30-6a0ae431333f",
   "metadata": {},
   "source": [
    "## Import dataset huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab574a30-36f7-46f2-8b5c-df8de3bc784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91f313-1b2c-4fdb-beb4-e3d1436c2286",
   "metadata": {},
   "source": [
    "## Import AST huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "91dfc551-2442-4274-a0a4-ed1281707bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "743c5bc6-a339-4c1f-ae24-9ec4641ca4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTForAudioClassification(\n",
       "  (audio_spectrogram_transformer): ASTModel(\n",
       "    (embeddings): ASTEmbeddings(\n",
       "      (patch_embeddings): ASTPatchEmbeddings(\n",
       "        (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ASTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ASTLayer(\n",
       "          (attention): ASTSdpaAttention(\n",
       "            (attention): ASTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ASTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ASTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ASTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): ASTMLPHead(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=768, out_features=527, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ast pretrained\n",
    "ast_huggingface = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de9e3-7f7f-40b2-911c-92d3d4013896",
   "metadata": {},
   "source": [
    "## Test pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4185ed5-b0e3-4b3c-82f1-4bc82a1c423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = ast_huggingface(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = ast_huggingface.config.id2label[predicted_class_ids]\n",
    "print(predicted_label)\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = ast_huggingface.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([ast_huggingface.config.label2id[target_label]])\n",
    "loss = ast_huggingface(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578e51-5352-4509-90ad-a8ee869c69c6",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcd6f0-f65a-44b9-b327-3096e50b4a89",
   "metadata": {},
   "source": [
    "## Retrieve Output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "187b8b8f-f1f4-4ffd-8ce1-e7c11fdd2eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASTModel(\n",
       "  (embeddings): ASTEmbeddings(\n",
       "    (patch_embeddings): ASTPatchEmbeddings(\n",
       "      (projection): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ASTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ASTLayer(\n",
       "        (attention): ASTSdpaAttention(\n",
       "          (attention): ASTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ASTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ASTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ASTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ASTModel\n",
    "import torch\n",
    "\n",
    "ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c0265fcd-366f-4fa5-9cd1-8204a8ffe4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1214, 768]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = ast_model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9207ad-fdde-42f1-b9b8-263d20db75e5",
   "metadata": {},
   "source": [
    "## Model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7892cd7b-906c-4c63-baaa-452420a9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AST_PromptTuning(nn.Module):\n",
    "\n",
    "    # dropout apply dropout after each prompt\n",
    "    # str = \"none\" --> only head tuning\n",
    "    def __init__(self, prompt_tokens: int = 5, prompt_dropout: float = 0.0, prompt_type: str = 'deep'):\n",
    "        super().__init__()\n",
    "\n",
    "        # load vit model\n",
    "        self.encoder = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        # hidden_size = depth of the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 384),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 192),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 96),\n",
    "            nn.Linear(384, 15)\n",
    "        )\n",
    "\n",
    "        # freeze\n",
    "        for n, p in self.encoder.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.prompt_type = prompt_type # \"shallow\" \"deep\" or None\n",
    "\n",
    "        if prompt_type is not None:\n",
    "\n",
    "            # prompt\n",
    "            self.prompt_tokens = prompt_tokens  # number of prompted tokens\n",
    "            self.prompt_dropout = nn.Dropout(prompt_dropout)\n",
    "            self.prompt_dim = self.encoder.config.hidden_size\n",
    "\n",
    "            # initiate prompt (random)\n",
    "            val = math.sqrt(6. / float(3 * reduce(mul, (self.encoder.config.patch_size, self.encoder.config.patch_size), 1) + self.prompt_dim))\n",
    "\n",
    "            # my vector of learnable parameters (how many (prompt_tokens) and dimension (prompt_dim))\n",
    "            self.prompt_embeddings = nn.Parameter(torch.zeros(1, self.prompt_tokens, self.prompt_dim))\n",
    "\n",
    "            # xavier_uniform initialization\n",
    "            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n",
    "\n",
    "            if self.prompt_type == 'deep':\n",
    "                self.total_d_layer = self.encoder.config.num_hidden_layers\n",
    "                self.deep_prompt_embeddings = nn.Parameter(\n",
    "                    # - 1 cause shallow already inserted\n",
    "                    torch.zeros(self.total_d_layer-1, self.prompt_tokens, self.prompt_dim)\n",
    "                )\n",
    "                # xavier_uniform initialization\n",
    "                nn.init.uniform_(self.deep_prompt_embeddings.data, -val, val)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # set train status for this class: disable all but the prompt-related modules\n",
    "        if mode:\n",
    "            # training:\n",
    "            self.encoder.eval()\n",
    "            if self.prompt_type is not None:\n",
    "              # enable dropout and batch normalization\n",
    "                self.prompt_dropout.train()\n",
    "        else:\n",
    "            # eval:\n",
    "            for module in self.children():\n",
    "                module.train(mode)\n",
    "\n",
    "    def incorporate_prompt(self, x, prompt_embeddings, n_prompt: int = 0):\n",
    "        # x shape: (batch size, n_tokens, hidden_dim)\n",
    "        # pompt_embeddings shape: (1, n_prompt, hidden_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # peek the class token, add prompts, add sequence\n",
    "\n",
    "        # concat prompts: (batch size, cls_token + n_prompt + n_patches, hidden_dim)\n",
    "        x = torch.cat((\n",
    "            x[:, :1, :],\n",
    "            self.prompt_dropout(prompt_embeddings.expand(B, -1, -1)),\n",
    "            x[:, (1+n_prompt):, :]\n",
    "        ), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "\n",
    "        # go through the encoder embeddings\n",
    "        x = self.encoder.embeddings(x)\n",
    "\n",
    "        # add prompts\n",
    "        x = self.incorporate_prompt(x, self.prompt_embeddings)\n",
    "\n",
    "        if self.prompt_type == 'deep':\n",
    "            # deep mode\n",
    "            x = model.encoder.encoder.layer[0](x)[0]\n",
    "            for i in range(1, self.total_d_layer):\n",
    "                x = self.incorporate_prompt(x, self.deep_prompt_embeddings[i-1], self.prompt_tokens)\n",
    "                x = model.encoder.encoder.layer[i](x)[0]\n",
    "        else:\n",
    "            # shallow mode\n",
    "            x = self.encoder.encoder(x)[\"last_hidden_state\"]\n",
    "\n",
    "        x = self.encoder.layernorm(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.prompt_type is not None:\n",
    "            x = self.forward_features(x)[:, 0, :]\n",
    "        else:\n",
    "          # pass x, take the classification token\n",
    "            x = self.encoder(x)[\"last_hidden_state\"][:, 0, :]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f5f66293-d648-4fb4-808d-1ef178a2e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST params: 86488335\n",
      "Head fine-tuning: 301071\n",
      "Shallow prompt-tuning: 304911\n",
      "Deep prompt-tuning: 347151\n"
     ]
    }
   ],
   "source": [
    "ast_prompt = AST_PromptTuning(prompt_type=None)\n",
    "# count number of parameters\n",
    "print(\"AST params:\", sum(p.numel() for p in ast_prompt.parameters()))\n",
    "# count number of trainable parameters\n",
    "print(\"Head fine-tuning:\", sum(p.numel() for p in ast_prompt.parameters() if p.requires_grad))\n",
    "ast_prompt_shallow = AST_PromptTuning(prompt_type='shallow')\n",
    "# count number of trainable parameters\n",
    "print(\"Shallow prompt-tuning:\", sum(p.numel() for p in ast_prompt_shallow.parameters() if p.requires_grad))\n",
    "ast_prompt_deep = AST_PromptTuning(prompt_type='deep')\n",
    "# count number of trainable parameters\n",
    "print(\"Deep prompt-tuning:\", sum(p.numel() for p in ast_prompt_deep.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a071ebf6-97fa-4c54-9831-12b5dbdd824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ast_prompt(inputs['input_values'])\n",
    "\n",
    "predicted_class_ids = torch.argmax(outputs, dim=-1).item()\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f0ca634f-8544-499d-b268-8383356231ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0868, 0.0701, 0.0472, 0.0732, 0.0544, 0.0472, 0.0497, 0.0480, 0.0695,\n",
       "         0.0493, 0.0593, 0.0988, 0.1038, 0.0229, 0.1197]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "softmax = F.softmax(outputs, dim=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bebb3-793a-4215-b567-97907f86307a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a87894-a860-4c30-a814-9119f3cedb47",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "370425c2-b8aa-44bb-bf95-07aa08e059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968417d-4032-4d6f-8d10-60063e4ff6db",
   "metadata": {},
   "source": [
    "## TUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7b89526e-dc00-4ffc-a07f-566c0d1f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "# from folder to PyTorch Dataset\n",
    "class TUT17(Dataset):\n",
    "    def __init__(self, root_dir, split = 'train', seed = 42, val_frac= 0.1, test_frac= 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # we use seed because every time we instantiate the dataset we shuffle all the data\n",
    "        # we call at least 3 times (train, validation, test) --> overlapping area\n",
    "        # with seed we are sure that the dataset is shuffled always in the same way\n",
    "        random.seed(seed)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        audio_names = os.listdir(os.path.join(root_dir, 'Audio'))\n",
    "        \n",
    "        num_val = int(len(audio_names)*val_frac)\n",
    "        num_test = int(len(audio_names)*test_frac)\n",
    "        num_train = len(audio_names) - num_val - num_test\n",
    "\n",
    "        random.shuffle(audio_names)\n",
    "    \n",
    "        # at this step we are only using images names - we are not using images\n",
    "        if split == 'train':\n",
    "            self.data = audio_names[:num_train]\n",
    "        elif split == 'val':\n",
    "            self.data = audio_names[num_train:num_train+num_val]\n",
    "        elif split == 'test':\n",
    "            self.data = audio_names[-num_test:]\n",
    "        else:\n",
    "          raise ValueError('Invalid split value.')\n",
    "    \n",
    "    # optional\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.root_dir, 'audio', self.data[idx])\n",
    "\n",
    "        audio_name = audio_path.split('/')[-1][6:].replace('\\\\', '/')\n",
    "\n",
    "        label_path = os.path.join(self.root_dir, 'labels\\evaluate.txt')\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            while line := f.readline():\n",
    "                if line.split('\\t')[0] == audio_name:\n",
    "                    f.close()\n",
    "                    audio, sample_rate = load_audio(audio_path)\n",
    "                    return {'audio': audio, 'sample_rate': sample_rate, 'label': line.split('\\t')[1][:-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d37473dd-0577-4cdc-97f6-de8d4662d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", split='train')\n",
    "val_dataset = TUT17(root_dir = 'c:/Users/cerru/Desktop/TUT17', split='val')\n",
    "test_dataset = TUT17(root_dir = 'c:/Users/cerru/Desktop/TUT17', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ee2c9675-88c7-4189-a173-da28bf87fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[ 7.3242e-03,  1.1750e-02,  1.0064e-02,  ..., -2.6589e-03,\n",
      "         -3.2216e-03,  0.0000e+00],\n",
      "        [-1.6393e-04, -2.6656e-04, -1.3281e-04,  ...,  4.0121e-04,\n",
      "         -8.6546e-06,  0.0000e+00],\n",
      "        [-2.5062e-04, -9.2742e-04, -1.6309e-03,  ..., -5.0162e-03,\n",
      "         -5.3743e-03,  0.0000e+00],\n",
      "        [ 8.9035e-04,  1.2040e-03,  1.9683e-03,  ...,  1.0775e-03,\n",
      "          3.8713e-04,  0.0000e+00]]), 'sample_rate': tensor([16000, 16000, 16000, 16000]), 'label': ['train', 'residential_area', 'residential_area', 'beach']}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=4, num_workers=0, shuffle=False, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=4, num_workers=0, shuffle=False, drop_last=True)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f191b1-4f7e-46e9-a07c-955634e586e8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548797e-da3e-4263-bda8-844ea5b40f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, feature_extractor, criterion, epochs, dev, lr=0.001, load_checkpoint = False, save_every = 10, save_path = 'weights'):\n",
    "    try:\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            \n",
    "        #Move model to CUDA\n",
    "        model = model.to(dev)\n",
    "\n",
    "        # create optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "        \n",
    "        # load checkpoints\n",
    "        if load_checkpoint:\n",
    "            if os.path.isfile(os.path.join(save_path,'weights.pt')):\n",
    "                print('Loading weights...')\n",
    "                # it is possible to load a state dict that doesn't match the networck architecture by passing asserting the strict mode\n",
    "                model.load_state_dict(torch.load(os.path.join(save_path,'weights.pt')))\n",
    "            if os.path.isfile(os.path.join(save_path,'optim.pt')):\n",
    "                print('Loading optimizer...')\n",
    "                optimizer.load_state_dict(torch.load(os.path.join(save_path,'optim.pt')))\n",
    "            print('Loading completed!')\n",
    "\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            \n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                #Select train() or eval() mode\n",
    "                torch.set_grad_enabled(split == 'train')\n",
    "                if split == 'train':\n",
    "                  model.train()\n",
    "                else:\n",
    "                  model.eval()\n",
    "                    \n",
    "                # Process each batch\n",
    "                for batch in loaders[split]:\n",
    "                    # Move to CUDA\n",
    "                    input_audio = batch['audio'].to(dev)\n",
    "                    sample_rate = batch['sample_rate'].to(dev)\n",
    "                    target = batch['label'].squeeze(1).to(dev)\n",
    "                    \n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Compute output\n",
    "                    ast_imput = feature_extractor(input_audio, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "                    output = model(ast_input)\n",
    "\n",
    "                    # Compute loss \n",
    "                    loss = criterion(output, target.long(), weight=weights)\n",
    "                    \n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    \n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    # Compute accuracy\n",
    "                    pred = torch.argmax(output,1)\n",
    "                    batch_accuracy = (pred == target).sum().item()/target.numel()\n",
    "                    \n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "\n",
    "                # checkpoint\n",
    "                if epoch%save_every == 0 and split == 'train':\n",
    "                    torch.save(model.state_dict(), os.path.join(save_path, 'weights.pt'))\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(save_path, 'optim.pt'))\n",
    "                \n",
    "                \n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cdde3f07-1812-4343-967c-982fdbc0ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bus': 108, 'residential_area': 108, 'car': 108, 'grocery_store': 108, 'train': 108, 'forest_path': 108, 'park': 108, 'library': 108, 'cafe/restaurant': 108, 'tram': 108, 'city_center': 108, 'office': 108, 'beach': 108, 'home': 108, 'metro_station': 108}\n"
     ]
    }
   ],
   "source": [
    "def conta_istanze_classi(path):\n",
    "    class_count = {}\n",
    "\n",
    "\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "\n",
    "            nome_audio, nome_classe = line.strip().split()\n",
    "\n",
    "            if nome_classe in class_count:\n",
    "                class_count[nome_classe] += 1\n",
    "            else:\n",
    "                class_count[nome_classe] = 1\n",
    "\n",
    "\n",
    "    return class_count\n",
    "\n",
    "print(conta_istanze_classi(\"c:/Users/cerru/Desktop/TUT17/labels/evaluate.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
