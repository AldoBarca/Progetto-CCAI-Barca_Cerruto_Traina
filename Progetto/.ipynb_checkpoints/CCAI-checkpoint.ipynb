{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d290d1-8777-435f-a5e2-123d0c6e436b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Library installation (only first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fd768-2fe9-4315-b472-23d52a5c921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce462e-0156-4751-aeba-df63a6cf37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a170de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45523688-2323-47c6-a4c3-74441a7d68b0",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab574a30-36f7-46f2-8b5c-df8de3bc784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e449586-6df0-4775-8c09-de9aa1c99c0f",
   "metadata": {},
   "source": [
    "# Import AST Pretrained and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf758e99-f37f-4e8c-9f30-6a0ae431333f",
   "metadata": {},
   "source": [
    "## Import dataset huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93ab4-bc00-4a86-9ee3-c6934a7b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_huggingface = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset_huggingface = dataset_huggingface.sort(\"id\")\n",
    "sampling_rate = dataset_huggingface.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91f313-1b2c-4fdb-beb4-e3d1436c2286",
   "metadata": {},
   "source": [
    "## Import AST huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfc551-2442-4274-a0a4-ed1281707bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c5bc6-a339-4c1f-ae24-9ec4641ca4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast pretrained\n",
    "ast_huggingface = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de9e3-7f7f-40b2-911c-92d3d4013896",
   "metadata": {},
   "source": [
    "## Test pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4185ed5-b0e3-4b3c-82f1-4bc82a1c423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = ast_huggingface(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = ast_huggingface.config.id2label[predicted_class_ids]\n",
    "print(predicted_label)\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = ast_huggingface.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([ast_huggingface.config.label2id[target_label]])\n",
    "loss = ast_huggingface(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578e51-5352-4509-90ad-a8ee869c69c6",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcd6f0-f65a-44b9-b327-3096e50b4a89",
   "metadata": {},
   "source": [
    "## Retrieve Output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b8b8f-f1f4-4ffd-8ce1-e7c11fdd2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0265fcd-366f-4fa5-9cd1-8204a8ffe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = ast_model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9207ad-fdde-42f1-b9b8-263d20db75e5",
   "metadata": {},
   "source": [
    "## Model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892cd7b-906c-4c63-baaa-452420a9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST_PromptTuning(nn.Module):\n",
    "    def __init__(self, prompt_tokens: int = 5, prompt_dropout: float = 0.0, prompt_type: str = 'deep'):\n",
    "        super().__init__()\n",
    "\n",
    "        # load AST model\n",
    "        self.encoder = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        # hidden_size = depth of the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 384),\n",
    "            nn.Linear(384, 15)\n",
    "        )\n",
    "\n",
    "        # freeze\n",
    "        for n, p in self.encoder.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.prompt_type = prompt_type # \"shallow\" \"deep\" or None\n",
    "\n",
    "        if prompt_type is not None:\n",
    "            # prompt\n",
    "            self.prompt_tokens = prompt_tokens  # number of prompted tokens\n",
    "            self.prompt_dropout = nn.Dropout(prompt_dropout)\n",
    "            self.prompt_dim = self.encoder.config.hidden_size\n",
    "\n",
    "            # initiate prompt (random)\n",
    "            val = math.sqrt(6. / float(3 * reduce(mul, (self.encoder.config.patch_size, self.encoder.config.patch_size), 1) + self.prompt_dim))\n",
    "\n",
    "            # my vector of learnable parameters (how many (prompt_tokens) and dimension (prompt_dim))\n",
    "            self.prompt_embeddings = nn.Parameter(torch.zeros(1, self.prompt_tokens, self.prompt_dim))\n",
    "\n",
    "            # xavier_uniform initialization\n",
    "            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n",
    "\n",
    "            if self.prompt_type == 'deep':\n",
    "                self.total_d_layer = self.encoder.config.num_hidden_layers\n",
    "                self.deep_prompt_embeddings = nn.Parameter(\n",
    "                    # - 1 cause shallow already inserted\n",
    "                    torch.zeros(self.total_d_layer-1, self.prompt_tokens, self.prompt_dim)\n",
    "                )\n",
    "                # xavier_uniform initialization\n",
    "                nn.init.uniform_(self.deep_prompt_embeddings.data, -val, val)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # set train status for this class: disable all but the prompt-related modules\n",
    "        if mode:\n",
    "            # training:\n",
    "            self.encoder.eval()\n",
    "            if self.prompt_type is not None:\n",
    "              # enable dropout and batch normalization\n",
    "                self.prompt_dropout.train()\n",
    "        else:\n",
    "            # eval:\n",
    "            for module in self.children():\n",
    "                module.train(mode)\n",
    "\n",
    "    def incorporate_prompt(self, x, prompt_embeddings, n_prompt: int = 0):\n",
    "        # x shape: (batch size, n_tokens, hidden_dim)\n",
    "        # pompt_embeddings shape: (1, n_prompt, hidden_dim)\n",
    "        B = x.shape[0]\n",
    "        # peek the class token, add prompts, add sequence\n",
    "        # concat prompts: (batch size, cls_token + n_prompt + n_patches, hidden_dim)\n",
    "        x = torch.cat((\n",
    "            x[:, :1, :],\n",
    "            self.prompt_dropout(prompt_embeddings.expand(B, -1, -1)),\n",
    "            x[:, (1+n_prompt):, :]\n",
    "        ), dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # go through the encoder embeddings\n",
    "        x = self.encoder.embeddings(x)\n",
    "        # add prompts\n",
    "        x = self.incorporate_prompt(x, self.prompt_embeddings)\n",
    "        if self.prompt_type == 'deep':\n",
    "            # deep mode\n",
    "            x = self.encoder.encoder.layer[0](x)[0]\n",
    "            for i in range(1, self.total_d_layer):\n",
    "                x = self.incorporate_prompt(x, self.deep_prompt_embeddings[i-1], self.prompt_tokens)\n",
    "                x = model.encoder.encoder.layer[i](x)[0]\n",
    "        else:\n",
    "            # shallow mode\n",
    "            x = self.encoder.encoder(x)[\"last_hidden_state\"]\n",
    "        x = self.encoder.layernorm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.prompt_type is not None:\n",
    "            x = self.forward_features(x)[:, 0, :]\n",
    "        else:\n",
    "          # pass x, take the classification token\n",
    "            x = self.encoder(x)[\"last_hidden_state\"][:, 0, :]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f66293-d648-4fb4-808d-1ef178a2e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_head_tuning = AST_PromptTuning(prompt_type=None)\n",
    "# count number of parameters\n",
    "print(\"AST params:\", sum(p.numel() for p in ast_head_tuning.parameters()))\n",
    "# count number of trainable parameters\n",
    "print(\"Head fine-tuning:\", sum(p.numel() for p in ast_prompt.parameters() if p.requires_grad))\n",
    "ast_shallow_tuning = AST_PromptTuning(prompt_type='shallow')\n",
    "# count number of trainable parameters\n",
    "print(\"Shallow prompt-tuning:\", sum(p.numel() for p in ast_shallow_tuning.parameters() if p.requires_grad))\n",
    "ast_deep_tuning = AST_PromptTuning(prompt_type='deep')\n",
    "# count number of trainable parameters\n",
    "print(\"Deep prompt-tuning:\", sum(p.numel() for p in ast_deep_tuning.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071ebf6-97fa-4c54-9831-12b5dbdd824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ast_prompt(inputs['input_values'])\n",
    "\n",
    "predicted_class_ids = torch.argmax(outputs, dim=-1).item()\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca634f-8544-499d-b268-8383356231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = F.softmax(outputs, dim=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bebb3-793a-4215-b567-97907f86307a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a87894-a860-4c30-a814-9119f3cedb47",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370425c2-b8aa-44bb-bf95-07aa08e059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoFeatureExtractor wants in input an array that contains the audio in format .flac --> this function convert a raw audio in .flac format\n",
    "def load_audio(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09e874-f2b2-4fdb-8f3c-a3d5dc8b5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(audio, sample_rate):\n",
    "    # pitch_shift\n",
    "    # audio = librosa.effects.pitch_shift(audio, sample_rate, n_steps = 2)\n",
    "    # add noise\n",
    "    # noise = np.random.randn(len(audio))\n",
    "    # audio = audio + (0.05 * noise)\n",
    "    #shift_time\n",
    "    shift = np.random.randint(len(audio) * 0.2)\n",
    "    audio = np.roll(audio, shift)\n",
    "    # amplificazione\n",
    "    ampl = 2.0\n",
    "    audio = audio * ampl\n",
    "    # random_crop\n",
    "    # crop_len = int(len(audio) 0.8)\n",
    "    # start = random.randint(0, len(audio) - crop_len)\n",
    "    # audio = audio[start : start + crop_len]\n",
    "    return audio, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f5bbb-fd4f-48b1-a7d0-70d4b69a10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the audio batch preprocessed\n",
    "def feature_extractor_batch_data(batch):\n",
    "    batch_feature_extractor = []\n",
    "    for index in range(0, len(batch[\"audio\"])):\n",
    "        output = feature_extractor(batch[\"audio\"][index], sampling_rate=batch[\"sample_rate\"][index], return_tensors=\"pt\")\n",
    "        batch_feature_extractor.append(output['input_values'])\n",
    "    # model wants in input a tensor with shape [num_batch, num_frame, num_mel]\n",
    "    batch_audio = torch.stack(batch_feature_extractor) # stacvk all the audio in a tensor in batch size\n",
    "    # prepare output\n",
    "    batch[\"audio\"] = batch_audio\n",
    "    return batch[\"audio\"], torch.tensor(batch[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcc138-624c-4058-ab46-a18e68d551f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test balanced dataset\n",
    "def count_class_presence_from_file(path):\n",
    "    class_count = {}\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            audio, label = line.strip().split()\n",
    "            if label in class_count:\n",
    "                class_count[label] += 1\n",
    "            else:\n",
    "                class_count[label] = 1\n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f992a4-9d06-4791-b2aa-72e790ed04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test balanced dataset\n",
    "def count_class_presence_from_list(list_audio):\n",
    "    class_count = {}\n",
    "    for item in list_audio:\n",
    "        audio, label = item.strip().split()\n",
    "        if label in class_count:\n",
    "            class_count[label] += 1\n",
    "        else:\n",
    "            class_count[label] = 1\n",
    "    return class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf9897-8351-4f76-89d5-4ed6db7bb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model wants in input an integer for each label --> this function create a dictionary that maps each label to an index\n",
    "def create_dict_label(path):\n",
    "    class_dict = {}\n",
    "    label_index = 0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            _, class_name = line.strip().split()\n",
    "            if class_name not in class_dict:\n",
    "                class_dict[class_name] = label_index\n",
    "                label_index += 1\n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd14ef-01bd-4d3d-beb0-bfcab70f8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve index of given label\n",
    "def from_label_to_index(label, dict_label):\n",
    "    return dict_label[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33ea50-3904-4a77-84bb-8bffebe2355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve audio file cross validation zenodo\n",
    "def create_file_list_split_fold(path, val_frac, task_train_percentage, num_fold = 4):\n",
    "    # retrieve alll files inside the folder (fold_x_train, fold_x_test, fold_x_evaluate)\n",
    "    folds_names = os.listdir(path)\n",
    "    \n",
    "    # discard test files (same of evaluate) --> train and test section\n",
    "    train_and_test_folds = [item for item in folds_names if item[-8:] != \"test.txt\"]\n",
    "\n",
    "    # retrieve all labels (only iterators purpose)\n",
    "    dict_label = create_dict_label(os.path.join(path, os.listdir(path)[0]))\n",
    "    labels = [key for key in dict_label.keys()]\n",
    "\n",
    "    # recreate folds\n",
    "    folds_indication = []\n",
    "    for i in range(0, num_fold):\n",
    "        # get num items train split\n",
    "        train_fold = [train for train in train_and_test_folds if train[-9:] == \"train.txt\"][0]\n",
    "        train_size = 0\n",
    "        with open(os.path.join(path, train_fold), 'r') as file:\n",
    "            for line in file:\n",
    "                train_size += 1\n",
    "    \n",
    "        # split dataset (task percentage)\n",
    "        num_train = int(train_size * task_train_percentage)\n",
    "    \n",
    "        # create train and split data\n",
    "        # we know that each split contains the same number of sample for each label\n",
    "        # recompute class percentage depending on task_train_percentage\n",
    "        \n",
    "        # retrieve class presence inside all split\n",
    "        class_presence = count_class_presence_from_file(os.path.join(path, train_fold))\n",
    "    \n",
    "        # recompute class presence\n",
    "        task_class_presence = dict([(label, math.floor(class_presence[label] * task_train_percentage)) for label in labels])\n",
    "    \n",
    "        train_file_row = []\n",
    "        with open(os.path.join(path, train_fold), 'r') as file:\n",
    "            # copy all row inside a list\n",
    "            train_file_row = [line for line in file]\n",
    "\n",
    "        test_fold = [test for test in train_and_test_folds if test[-9:] != \"train.txt\"][0]\n",
    "        test_file_row = []\n",
    "        with open(os.path.join(path, test_fold), 'r') as file:\n",
    "            # copy all row inside a list\n",
    "            test_file_row = [line for line in file]\n",
    "    \n",
    "        # take task_class_presence row for each label - shift inside list of class_presence\n",
    "        train_file = []\n",
    "        # create n subset (1 for each label)\n",
    "        for index in range(0, len(labels)):\n",
    "            # selected label: labels[index]\n",
    "            # retrieve m sample of each label (m: task_class_presence)\n",
    "            index_rows = random.sample(range(index * class_presence[labels[index]], class_presence[labels[index]] * (index + 1)), task_class_presence[labels[index]])\n",
    "            \n",
    "            train_rows = [train_file_row[file_index] for file_index in index_rows]\n",
    "            [train_file.append(item) for item in train_rows]\n",
    "    \n",
    "        random.shuffle(train_file)\n",
    "\n",
    "        # split train in train and validation\n",
    "        num_train = len(train_file)\n",
    "        num_val = int(num_train * val_frac)\n",
    "\n",
    "        # separate audio from label\n",
    "        train_audio_list = [item.strip().split()[0][6:] for item in train_file[:num_train - num_val]]\n",
    "        train_label_list = [item.strip().split()[1] for item in train_file[:num_train - num_val]]\n",
    "        val_audio_list = [item.strip().split()[0][6:] for item in train_file[num_train - num_val:]]\n",
    "        val_label_list = [item.strip().split()[1] for item in train_file[num_train - num_val:]]\n",
    "        test_audio_list = [item.strip().split()[0][6:] for item in test_file_row]\n",
    "        test_label_list = [item.strip().split()[1] for item in test_file_row]\n",
    "        \n",
    "        \n",
    "        folds_indication.append({\n",
    "            'train_audio_list': train_audio_list,\n",
    "            'train_label_list': train_label_list,\n",
    "            'val_audio_list': val_audio_list,\n",
    "            'val_label_list': val_label_list,\n",
    "            'test_audio_list': test_audio_list,\n",
    "            'test_label_list': test_label_list\n",
    "        })\n",
    "    return folds_indication, dict_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968417d-4032-4d6f-8d10-60063e4ff6db",
   "metadata": {},
   "source": [
    "## TUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89526e-dc00-4ffc-a07f-566c0d1f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from folder to PyTorch Dataset\n",
    "class TUT17(Dataset):\n",
    "    # root_dir: TUT17 folder\n",
    "    # audio_folder: folder that contains all audio (no division)\n",
    "    # split: tell which part of the dataset i'm using\n",
    "    def __init__(self, root_dir, audio_folder, fold_specification, class_dict, split = 'train'):\n",
    "        super().__init__()\n",
    "        # store path audio\n",
    "        self.audio_path = os.path.join(root_dir, audio_folder)\n",
    "        # store dict label\n",
    "        self.class_dict = class_dict     \n",
    "        # split dataset (audio names)\n",
    "        if split == 'train':\n",
    "            self.data = fold_specification['train_audio_list']\n",
    "            self.label = fold_specification['train_label_list']\n",
    "        elif split == 'val':\n",
    "            self.data = fold_specification['val_audio_list']\n",
    "            self.label = fold_specification['val_label_list']\n",
    "        elif split == 'test':\n",
    "            self.data = fold_specification['test_audio_list']\n",
    "            self.label = fold_specification['test_label_list']\n",
    "        else:\n",
    "          raise ValueError('Invalid split value.')\n",
    "    \n",
    "    # optional\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_path, self.data[idx])\n",
    "        audio, sample_rate = load_audio(audio_path)\n",
    "        audio, sample_rate = data_augmentation(audio, sample_rate)\n",
    "        return {'audio': audio, 'sample_rate': sample_rate, 'label': from_label_to_index(self.label[idx], self.class_dict)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb7d7e-76ae-40cc-bd36-9df61cfdbf08",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313650d0-9f4b-48aa-9119-c1becea13199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # reset\n",
    "        optimizer.zero_grad()\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "        # compute loss and update parameters\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfcca-8a9a-410d-80e0-a2e2998a51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(val_loader):\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "        # compute loss\n",
    "        loss = criterion(output, labels)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd60963-7457-492a-9392-4b5f1b7dba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    labels_l = []\n",
    "    predictions_l = []\n",
    "    for batch in tqdm(test_loader):\n",
    "         # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        # compute accuracy\n",
    "        labels_l.append(labels)\n",
    "        predictions_l.append(predictions)\n",
    "    labels = torch.cat(labels_l, dim=0)\n",
    "    predictions = torch.cat(predictions_l, dim=0)\n",
    "    accuracy = (predictions == labels).sum().item() / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fdab4-5013-4fa0-9a75-c95421dc8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(path, val_frac, task_train_percentage, model_type, folder_checkpoints, n_epochs: int = 10): \n",
    "    # select device\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # instantiate criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # retrieve fold\n",
    "    folds_indication, class_dict = create_file_list_split_fold(path, val_frac, task_train_percentage * 10 / 100)\n",
    "    # create epochs statistics of loss\n",
    "    epoch_statistics = []\n",
    "    for epoch in range(n_epochs):\n",
    "        folds_loss = []\n",
    "        for fold in range(len(folds_indication)):\n",
    "            folds_loss.append({\n",
    "                'train_loss': 0.0,\n",
    "                'val_loss': 0.0\n",
    "            })\n",
    "        epoch_statistics.append(folds_loss)\n",
    "    # create folds statistic of accuracy\n",
    "    folds_accuracy = []    \n",
    "    # for each fold train the model for n_epochs\n",
    "    sum_accuracy = 0.0\n",
    "    for fold in tqdm(range(len(folds_indication))):\n",
    "        # instantiate model\n",
    "        if model_type == 0:\n",
    "            model = AST_PromptTuning(prompt_type = None)\n",
    "        elif model_type == 1:\n",
    "            model = AST_PromptTuning(prompt_type = 'shallow')\n",
    "        elif model_type == 2:\n",
    "            model = AST_PromptTuning(prompt_type = 'deep')\n",
    "        else:\n",
    "          raise ValueError('Invalid Model type')\n",
    "        # instantiate optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        # instantiate dataset\n",
    "        train_dataset = TUT17(\"c:/Users/cerru/Desktop/TUT17\", \"Audio\", folds_indication[fold], class_dict, split = 'train')\n",
    "        val_dataset = TUT17(\"c:/Users/cerru/Desktop/TUT17\", \"Audio\", folds_indication[fold], class_dict, split = 'val')\n",
    "        test_dataset = TUT17(\"c:/Users/cerru/Desktop/TUT17\", \"Audio\", folds_indication[fold], class_dict, split = 'test')\n",
    "        # instantiate data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "        # train model\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # execute train\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, dev)\n",
    "            epoch_statistics[epoch][fold]['train_loss'] = train_loss\n",
    "            # execute validation\n",
    "            val_loss = validation(model, val_loader, criterion, dev)\n",
    "            epoch_statistics[epoch][fold]['val_loss'] = val_loss\n",
    "\n",
    "            # print single stats\n",
    "            print(f'Epoch {epoch + 1}/{n_epochs} Fold {fold + 1} : Train Loss {train_loss:.4f} : Val Loss {val_loss:.4f}')\n",
    "        # test model\n",
    "        accuracy = test(model, test_loader, criterion, dev)\n",
    "        folds_accuracy.append(accuracy)\n",
    "        print(f'Fold {fold + 1} : Accuracy {accuracy:.4f}')\n",
    "        # store parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, os.path.join(folder_checkpoints, \"model_type_\" + str(model_type) + \"_train_frac_\" + str(task_train_percentage * 100) + \".pth\"))\n",
    "    return folds_accuracy, epoch_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1c4c7-508c-4feb-bb46-c39b899bb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_cross_validation(epoch_statistics, folds_accuracy):\n",
    "    # mean performance of each fold for each epoch\n",
    "    epoch_train_loss_mean = []\n",
    "    epoch_val_loss_mean = []\n",
    "    for epoch in range(len(epoch_statistics)):\n",
    "        sum_train_loss = 0.0\n",
    "        sum_val_loss = 0.0\n",
    "        for fold in range(len(epoch_statistics[epoch])):\n",
    "            sum_train_loss += epoch_statistics[epoch][fold]['train_loss']\n",
    "            sum_val_loss += epoch_statistics[epoch][fold]['val_loss']\n",
    "        epoch_train_loss_mean.append(sum_train_loss / len(epoch_statistics[epoch]))\n",
    "        epoch_val_loss_mean.append(sum_val_loss / len(epoch_statistics[epoch]))\n",
    "    mean_accuracy = np.mean(folds_accuracy)\n",
    "    return epoch_train_loss_mean, epoch_val_loss_mean, mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f191b1-4f7e-46e9-a07c-955634e586e8",
   "metadata": {},
   "source": [
    "## Task execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494a9a6-2b1d-487a-9f6d-996bb7d7777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = {\n",
    "    'dataset_10_only_head': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_50_only_head': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_100_only_head': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_10_shallow_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_50_shallow_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_100_shallow_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_10_deep_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_50_deep_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "    'dataset_100_deep_prompt': {\n",
    "            'epoch_train_loss_mean': [],\n",
    "            'epoch_val_loss_mean': [],\n",
    "            'mean_accuracy': 0.0\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b9aa6-220b-4c9c-8d0f-71633f4fdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_10_only_head_processed_data.npy\")):\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.1, 0, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_only_head_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_only_head_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d07db0-10a7-44d9-bbcc-5cd0643413c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_50_only_head_processed_data.npy\")):\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.5, 0, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_only_head_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_only_head_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3461d6c-1a7e-4b70-8022-0c0e2b5a1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_100_only_head_processed_data.npy\")):\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 1, 0, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_only_head_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_only_head_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825a53c-fdfc-4c1c-b2df-c10a5d5e51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_10_shallow_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.1, 1, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_shallow_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_shallow_prompt_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeeea98-b4f6-4bbb-9356-ef5987d1a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_50_shallow_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.5, 1, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_shallow_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_shallow_prompt_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52ecce-a232-49a9-a8f9-5852df9b9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_100_shallow_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 1, 1, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_shallow_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_shallow_prompt_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefad56-8004-45a0-87a0-0080909669b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_10_deep_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.1, 2, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_deep_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_10_deep_prompt_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958b66c-7fce-4b6e-8b44-5e2ba31d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_50_deep_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 0.5, 2, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_deep_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_50_deep_prompt_processed_data.npy'), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8ba78-27f4-440e-a4a3-c13cbf1ef140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(\"./models_metrics\", \"dataset_100_deep_prompt_processed_data.npy\")):\n",
    "    # da fare\n",
    "    folds_accuracy, epoch_statistics = cross_validation(\"c:/Users/cerru/Desktop/TUT17/Fold\", 0.1, 1, 2, \"c:/Users/cerru/Desktop/models_checkpoints\")\n",
    "    # Save raw data\n",
    "    metrics = {\n",
    "        'folds_accuracy': folds_accuracy,\n",
    "        'epoch_statistics': epoch_statistics\n",
    "    }\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_deep_prompt_raw_data.npy'), metrics)\n",
    "    # process data\n",
    "    etlm, evlm, ma = compute_statistics_cross_validation(metrics['epoch_statistics'], metrics['folds_accuracy'])\n",
    "    processed_data = {\n",
    "        'epoch_train_loss_mean': etlm,\n",
    "        'epoch_val_loss_mean': etlm,\n",
    "        'mean_accuracy': ma\n",
    "    }\n",
    "    # save processed data\n",
    "    np.save(os.path.join(\"./models_metrics\", 'dataset_100_deep_prompt_processed_data.npy'), processed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
