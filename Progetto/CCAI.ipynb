{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c5dcf1-fd7f-498b-a627-f95b3d1f7a28",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d290d1-8777-435f-a5e2-123d0c6e436b",
   "metadata": {},
   "source": [
    "## Library installation (only first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40eceac-2dd3-4eba-bfbc-8b2a2cfb7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fd768-2fe9-4315-b472-23d52a5c921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce462e-0156-4751-aeba-df63a6cf37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fff90d-213e-48ab-9eb4-34bb2f5bf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a170de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45523688-2323-47c6-a4c3-74441a7d68b0",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab574a30-36f7-46f2-8b5c-df8de3bc784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification, ASTModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e449586-6df0-4775-8c09-de9aa1c99c0f",
   "metadata": {},
   "source": [
    "# Import AST Pretrained and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf758e99-f37f-4e8c-9f30-6a0ae431333f",
   "metadata": {},
   "source": [
    "## Import dataset huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93ab4-bc00-4a86-9ee3-c6934a7b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_huggingface = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
    "dataset_huggingface = dataset_huggingface.sort(\"id\")\n",
    "sampling_rate = dataset_huggingface.features[\"audio\"].sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91f313-1b2c-4fdb-beb4-e3d1436c2286",
   "metadata": {},
   "source": [
    "## Import AST huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfc551-2442-4274-a0a4-ed1281707bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c5bc6-a339-4c1f-ae24-9ec4641ca4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ast pretrained\n",
    "ast_huggingface = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574de9e3-7f7f-40b2-911c-92d3d4013896",
   "metadata": {},
   "source": [
    "## Test pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4185ed5-b0e3-4b3c-82f1-4bc82a1c423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = ast_huggingface(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = ast_huggingface.config.id2label[predicted_class_ids]\n",
    "print(predicted_label)\n",
    "\n",
    "# compute loss - target_label is e.g. \"down\"\n",
    "target_label = ast_huggingface.config.id2label[0]\n",
    "inputs[\"labels\"] = torch.tensor([ast_huggingface.config.label2id[target_label]])\n",
    "loss = ast_huggingface(**inputs).loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578e51-5352-4509-90ad-a8ee869c69c6",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcd6f0-f65a-44b9-b327-3096e50b4a89",
   "metadata": {},
   "source": [
    "## Retrieve Output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b8b8f-f1f4-4ffd-8ce1-e7c11fdd2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "ast_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0265fcd-366f-4fa5-9cd1-8204a8ffe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = ast_model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9207ad-fdde-42f1-b9b8-263d20db75e5",
   "metadata": {},
   "source": [
    "## Model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892cd7b-906c-4c63-baaa-452420a9edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AST_PromptTuning(nn.Module):\n",
    "\n",
    "    # dropout apply dropout after each prompt\n",
    "    # str = \"none\" --> only head tuning\n",
    "    def __init__(self, prompt_tokens: int = 5, prompt_dropout: float = 0.0, prompt_type: str = 'deep'):\n",
    "        super().__init__()\n",
    "\n",
    "        # load vit model\n",
    "        self.encoder = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "        # hidden_size = depth of the model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 384),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 192),\n",
    "            # nn.Linear(self.encoder.config.hidden_size, 96),\n",
    "            nn.Linear(384, 15)\n",
    "        )\n",
    "\n",
    "        # freeze\n",
    "        for n, p in self.encoder.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.prompt_type = prompt_type # \"shallow\" \"deep\" or None\n",
    "\n",
    "        if prompt_type is not None:\n",
    "\n",
    "            # prompt\n",
    "            self.prompt_tokens = prompt_tokens  # number of prompted tokens\n",
    "            self.prompt_dropout = nn.Dropout(prompt_dropout)\n",
    "            self.prompt_dim = self.encoder.config.hidden_size\n",
    "\n",
    "            # initiate prompt (random)\n",
    "            val = math.sqrt(6. / float(3 * reduce(mul, (self.encoder.config.patch_size, self.encoder.config.patch_size), 1) + self.prompt_dim))\n",
    "\n",
    "            # my vector of learnable parameters (how many (prompt_tokens) and dimension (prompt_dim))\n",
    "            self.prompt_embeddings = nn.Parameter(torch.zeros(1, self.prompt_tokens, self.prompt_dim))\n",
    "\n",
    "            # xavier_uniform initialization\n",
    "            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n",
    "\n",
    "            if self.prompt_type == 'deep':\n",
    "                self.total_d_layer = self.encoder.config.num_hidden_layers\n",
    "                self.deep_prompt_embeddings = nn.Parameter(\n",
    "                    # - 1 cause shallow already inserted\n",
    "                    torch.zeros(self.total_d_layer-1, self.prompt_tokens, self.prompt_dim)\n",
    "                )\n",
    "                # xavier_uniform initialization\n",
    "                nn.init.uniform_(self.deep_prompt_embeddings.data, -val, val)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        # set train status for this class: disable all but the prompt-related modules\n",
    "        if mode:\n",
    "            # training:\n",
    "            self.encoder.eval()\n",
    "            if self.prompt_type is not None:\n",
    "              # enable dropout and batch normalization\n",
    "                self.prompt_dropout.train()\n",
    "        else:\n",
    "            # eval:\n",
    "            for module in self.children():\n",
    "                module.train(mode)\n",
    "\n",
    "    def incorporate_prompt(self, x, prompt_embeddings, n_prompt: int = 0):\n",
    "        # x shape: (batch size, n_tokens, hidden_dim)\n",
    "        # pompt_embeddings shape: (1, n_prompt, hidden_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # peek the class token, add prompts, add sequence\n",
    "\n",
    "        # concat prompts: (batch size, cls_token + n_prompt + n_patches, hidden_dim)\n",
    "        x = torch.cat((\n",
    "            x[:, :1, :],\n",
    "            self.prompt_dropout(prompt_embeddings.expand(B, -1, -1)),\n",
    "            x[:, (1+n_prompt):, :]\n",
    "        ), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "\n",
    "        # go through the encoder embeddings\n",
    "        x = self.encoder.embeddings(x)\n",
    "\n",
    "        # add prompts\n",
    "        x = self.incorporate_prompt(x, self.prompt_embeddings)\n",
    "\n",
    "        if self.prompt_type == 'deep':\n",
    "            # deep mode\n",
    "            x = self.encoder.encoder.layer[0](x)[0]\n",
    "            for i in range(1, self.total_d_layer):\n",
    "                x = self.incorporate_prompt(x, self.deep_prompt_embeddings[i-1], self.prompt_tokens)\n",
    "                x = model.encoder.encoder.layer[i](x)[0]\n",
    "        else:\n",
    "            # shallow mode\n",
    "            x = self.encoder.encoder(x)[\"last_hidden_state\"]\n",
    "\n",
    "        x = self.encoder.layernorm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.prompt_type is not None:\n",
    "            x = self.forward_features(x)[:, 0, :]\n",
    "        else:\n",
    "          # pass x, take the classification token\n",
    "            x = self.encoder(x)[\"last_hidden_state\"][:, 0, :]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f66293-d648-4fb4-808d-1ef178a2e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt = AST_PromptTuning(prompt_type=None)\n",
    "# count number of parameters\n",
    "print(\"AST params:\", sum(p.numel() for p in ast_prompt.parameters()))\n",
    "# count number of trainable parameters\n",
    "print(\"Head fine-tuning:\", sum(p.numel() for p in ast_prompt.parameters() if p.requires_grad))\n",
    "ast_prompt_shallow = AST_PromptTuning(prompt_type='shallow')\n",
    "# count number of trainable parameters\n",
    "print(\"Shallow prompt-tuning:\", sum(p.numel() for p in ast_prompt_shallow.parameters() if p.requires_grad))\n",
    "ast_prompt_deep = AST_PromptTuning(prompt_type='deep')\n",
    "# count number of trainable parameters\n",
    "print(\"Deep prompt-tuning:\", sum(p.numel() for p in ast_prompt_deep.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071ebf6-97fa-4c54-9831-12b5dbdd824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset_huggingface[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ast_prompt(inputs['input_values'])\n",
    "\n",
    "predicted_class_ids = torch.argmax(outputs, dim=-1).item()\n",
    "predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca634f-8544-499d-b268-8383356231ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = F.softmax(outputs, dim=1)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bebb3-793a-4215-b567-97907f86307a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a87894-a860-4c30-a814-9119f3cedb47",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370425c2-b8aa-44bb-bf95-07aa08e059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoFeatureExtractor wants in input an array that contains the audio in format .flac --> this function convert a raw audio in .flac format\n",
    "def load_audio(audio_path):\n",
    "    audio, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# this function returns the audio batch preprocessed\n",
    "def feature_extractor_batch_data(batch):\n",
    "    batch_feature_extractor = []\n",
    "    for index in range(0, len(batch[\"audio\"])):\n",
    "        output = feature_extractor(batch[\"audio\"][index], sampling_rate=batch[\"sample_rate\"][index], return_tensors=\"pt\")\n",
    "        batch_feature_extractor.append(output['input_values'])\n",
    "    # model wants in input a tensor with shape [num_batch, num_frame, num_mel]\n",
    "    batch_audio = torch.stack(batch_feature_extractor) # stacvk all the audio in a tensor in batch size\n",
    "    # prepare output\n",
    "    batch[\"audio\"] = batch_audio\n",
    "    return batch[\"audio\"], torch.tensor(batch[\"label\"])\n",
    "\n",
    "# test balanced dataset\n",
    "def count_class_presence(path):\n",
    "    class_count = {}\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            audio, label = line.strip().split()\n",
    "            if label in class_count:\n",
    "                class_count[label] += 1\n",
    "            else:\n",
    "                class_count[label] = 1\n",
    "    return class_count\n",
    "\n",
    "# model wants in input an integer for each label --> this function create a dictionary that maps each label to an index\n",
    "def create_dict_label(path):\n",
    "    class_dict = {}\n",
    "    label_index = 0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            _, class_name = line.strip().split()\n",
    "            if class_name not in class_dict:\n",
    "                class_dict[class_name] = label_index\n",
    "                label_index += 1\n",
    "    return class_dict\n",
    "\n",
    "# retrieve index of given label\n",
    "def from_label_to_index(label, dict_label):\n",
    "    return dict_label[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968417d-4032-4d6f-8d10-60063e4ff6db",
   "metadata": {},
   "source": [
    "## TUT17 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89526e-dc00-4ffc-a07f-566c0d1f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from folder to PyTorch Dataset\n",
    "class TUT17(Dataset):\n",
    "    def __init__(self, root_dir, audio_folder, label_folder, label_filename, split = 'train', seed = 42, val_frac= 0.1, test_frac= 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # we use seed because every time we instantiate the dataset we shuffle all the data\n",
    "        # we call at least 3 times (train, validation, test) --> overlapping area\n",
    "        # with seed we are sure that the dataset is shuffled always in the same way\n",
    "        random.seed(seed)\n",
    "        \n",
    "        # store path audio and label\n",
    "        self.label_path = os.path.join(os.path.join(root_dir, label_folder), label_filename)\n",
    "        self.audio_path = os.path.join(root_dir, audio_folder)\n",
    "\n",
    "        # create dict label\n",
    "        self.class_dict = create_dict_label(self.label_path)\n",
    "        # retrive audio files\n",
    "        audio_names = os.listdir(self.audio_path)\n",
    "\n",
    "        # split dataset (percentage)\n",
    "        num_val = int(len(audio_names)*val_frac)\n",
    "        num_test = int(len(audio_names)*test_frac)\n",
    "        num_train = len(audio_names) - num_val - num_test\n",
    "\n",
    "        random.shuffle(audio_names)\n",
    "    \n",
    "        # split dataset (indexes)\n",
    "        if split == 'train':\n",
    "            self.data = audio_names[:num_train]\n",
    "        elif split == 'val':\n",
    "            self.data = audio_names[num_train:num_train+num_val]\n",
    "        elif split == 'test':\n",
    "            self.data = audio_names[-num_test:]\n",
    "        else:\n",
    "          raise ValueError('Invalid split value.')\n",
    "    \n",
    "    # optional\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_path, self.data[idx])\n",
    "        audio_name = audio_path.split('/')[-1][6:].replace('\\\\', '/').split('/')[-1]\n",
    "        with open(self.label_path, \"r\") as f:\n",
    "            while line := f.readline():\n",
    "                if line.split('\\t')[0][6:] == audio_name:\n",
    "                    f.close()\n",
    "                    audio, sample_rate = load_audio(audio_path)\n",
    "                    return {'audio': audio, 'sample_rate': sample_rate, 'label': from_label_to_index(line.split('\\t')[1][:-1], self.class_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37473dd-0577-4cdc-97f6-de8d4662d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='train')\n",
    "val_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='val')\n",
    "test_dataset = TUT17(root_dir = \"c:/Users/cerru/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='test')\n",
    "'''\n",
    "train_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='train')\n",
    "val_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='val')\n",
    "test_dataset = TUT17(root_dir = \"C:/Users/aldob/Desktop/TUT17\", audio_folder = \"Audio\", label_folder  = \"labels\", label_filename = \"evaluate.txt\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c9675-88c7-4189-a173-da28bf87fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=4, num_workers=0, shuffle=False, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=4, num_workers=0, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2902f-f1b4-4f21-a63e-ff2c90cccf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f191b1-4f7e-46e9-a07c-955634e586e8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313650d0-9f4b-48aa-9119-c1becea13199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfcca-8a9a-410d-80e0-a2e2998a51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(val_loader):\n",
    "        # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd60963-7457-492a-9392-4b5f1b7dba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    running_loss = 0.0\n",
    "    labels_l = []\n",
    "    predictions_l = []\n",
    "    for batch in tqdm(test_loader):\n",
    "         # preprocess input AST feature extractor\n",
    "        audio_list, labels = feature_extractor_batch_data(batch)\n",
    "\n",
    "        # send input and labels to CUDA\n",
    "        audio_list = audio_list.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Compute output\n",
    "        output = model(audio_list.squeeze())\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        \n",
    "        # compute accuracy\n",
    "        labels_l.append(labels)\n",
    "        predictions_l.append(predictions)\n",
    "\n",
    "    labels = torch.cat(labels_l, dim=0)\n",
    "    predictions = torch.cat(predictions_l, dim=0)\n",
    "    \n",
    "    accuracy = (predictions == labels).sum().item() / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832ebb-acda-4849-87e0-9c144c0157f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, train_mode,n_epochs: int = 10):\n",
    "    if train_mode==0:\n",
    "        last_model_path = os.path.join(\"../models_checkpoint\", 'ast.pth')\n",
    "    elif train_mode==1:\n",
    "        last_model_path = os.path.join(\"../models_checkpoint\", 'ast_shallow.pth')\n",
    "    elif train_mode==2:\n",
    "        last_model_path = os.path.join(\"../models_checkpoint\", 'ast_deep.pth')\n",
    "\n",
    "\n",
    "    if  os.path.isfile(last_model_path):\n",
    "        checkpoint = torch.load(last_model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        if epoch%2 == 0:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch, \n",
    "                    },last_model_path)\n",
    "\n",
    "                    #torch.save(model.state_dict(), os.path.join(last_model_path, 'weights.pt'))\n",
    "                    #torch.save(optimizer.state_dict(), os.path.join(last_model_path, 'optim.pt'))\n",
    "        print(f'Epoch {epoch+1}/{n_epochs} : Train Loss {train_loss:.4f} : Val Loss {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c893d9-5497-4b25-9a82-589420bdddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt = AST_PromptTuning(prompt_type=None)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_normal = torch.optim.Adam(ast_prompt.parameters(), lr=0.0001)\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "\n",
    "train(ast_prompt, train_loader, val_loader, criterion, optimizer_normal, dev,train_mode=0)\n",
    "test(ast_prompt, test_loader, criterion, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d3093-6b94-4dce-ad21-bd9810422d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt_shallow = AST_PromptTuning(prompt_type='shallow')\n",
    "\n",
    "optimizer_shallow = torch.optim.Adam(ast_prompt_shallow.parameters(), lr=0.0001)\n",
    "\n",
    "train(ast_prompt, train_loader, val_loader, criterion, optimizer_shallow, dev,train_mode=1)\n",
    "test(ast_prompt, test_loader, criterion, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c6751-5451-4dac-9014-79c417c3105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_prompt_deep = AST_PromptTuning(prompt_type='deep')\n",
    "\n",
    "optimizer_deep = torch.optim.Adam(ast_prompt_deep.parameters(), lr=0.0001)\n",
    "\n",
    "train(ast_prompt_deep, train_loader, val_loader, criterion, optimizer_deep, dev,train_mode=2)\n",
    "test(ast_prompt_deep, test_loader, criterion, dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
